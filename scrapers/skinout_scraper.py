def main():
    """
    Funci√≥n principal para ejecutar el scraper
    NUEVO: Carga autom√°tica de proxies desde proxy.txt con logs detallados
    """
    print("=== SkinOut Scraper V2.1 - Logs de Proxy Mejorados ===")
    print("Nuevas caracter√≠sticas:")
    print("- ‚úÖ Carga autom√°tica desde BOT-VCSGO-BETA-V2/proxy.txt")
    print("- ‚úÖ Logs detallados de uso de proxies en tiempo real")
    print("- ‚úÖ Verificaci√≥n de rotaci√≥n de proxies en errores 429")
    print("- ‚úÖ Monitoreo de workers y paralelizaci√≥n")
    print("- ‚úÖ Estad√≠sticas completas de conexiones proxy")
    
    # Configuraci√≥n de proxy con carga autom√°tica
    use_proxy_input = input("\n¬øUsar proxy? (Y/n): ").lower()
    use_proxy = use_proxy_input != 'n'  # Por defecto True
    
    # Mostrar informaci√≥n de carga de proxies
    if use_proxy:
        print("\nüìÇ Verificando archivo de proxies...")
        # Verificar si existe el archivo antes de crear el scraper
        current_file = Path(__file__)
        """
SkinOut Scraper - BOT-vCSGO-Beta V2 - Mejorado

Scraper migrado desde BOT-vCSGO-Beta para SkinOut.gg
- Migrado desde core/scrapers/skinout_scraper.py de BOT-vCSGO-Beta
- API-based usando endpoints paginados de SkinOut
- Cliente optimizado con soporte para paralelizaci√≥n
- Sistema de reintentos autom√°ticos por p√°gina
- NUEVO: Proxy por defecto con 230k proxies disponibles
- NUEVO: Rate limiting inteligente sin proxy
- NUEVO: Reintentos indefinidos con backoff exponencial
"""

from typing import List, Dict, Optional
import sys
from pathlib import Path
import time
import random
import requests  # Importar requests para uso directo con proxies
from concurrent.futures import ThreadPoolExecutor, as_completed
import os

# Agregar el directorio core al path
sys.path.append(str(Path(__file__).parent.parent))

from core.base_scraper import BaseScraper


def cargar_proxies_desde_archivo() -> Optional[List[str]]:
    """
    Carga la lista de proxies desde BOT-VCSGO-BETA-V2/proxy.txt
    
    Returns:
        Lista de proxies o None si no se puede cargar
    """
    # Obtener la ruta del archivo actual
    current_file = Path(__file__)
    # Ir al directorio padre (BOT-VCSGO-BETA-V2)
    project_root = current_file.parent.parent
    proxy_file = project_root / "proxy.txt"
    
    try:
        if proxy_file.exists():
            with open(proxy_file, 'r', encoding='utf-8') as f:
                proxies = [line.strip() for line in f if line.strip()]
            return proxies
        else:
            print(f"‚ùå Archivo de proxies no encontrado: {proxy_file}")
            return None
    except Exception as e:
        print(f"‚ùå Error cargando proxies desde {proxy_file}: {e}")
        return None


class SkinoutClient:
    """
    Cliente para SkinOut.gg (migrado desde BOT-vCSGO-Beta)
    
    Caracter√≠sticas mejoradas:
    - API endpoint: https://skinout.gg/api/market/items?page=N
    - Paginaci√≥n autom√°tica
    - Sistema de reintentos indefinidos con backoff inteligente
    - Soporte para paralelizaci√≥n masiva con proxies (1000 workers)
    - Rate limiting inteligente sin proxy
    - Rotaci√≥n autom√°tica de proxies en errores 429
    - NUEVO: Sistema de rotaci√≥n de proxies del c√≥digo original
    """
    
    def __init__(self, session, logger=None, use_proxy=True, max_workers=None, proxy_count=0, proxy_list=None):
        """
        Inicializa el cliente SkinOut
        
        Args:
            session: Sesi√≥n de requests del BaseScraper
            logger: Logger para registro de eventos
            use_proxy: Si usar proxy (por defecto True)
            max_workers: N√∫mero m√°ximo de threads (auto-configurado seg√∫n proxy)
            proxy_count: N√∫mero de proxies disponibles (para logs)
            proxy_list: Lista de proxies para rotaci√≥n manual
        """
        self.session = session
        self.logger = logger
        self.use_proxy = use_proxy
        self.proxy_count = proxy_count
        
        # Sistema de rotaci√≥n de proxies del c√≥digo original
        self.proxy_list = proxy_list or []
        self.current_proxy_index = 0
        
        # Configuraci√≥n seg√∫n modo proxy
        if use_proxy:
            # Modo proxy: m√°xima velocidad
            self.max_workers = max_workers or 1000  # Masivo con 230k proxies
            self.retry_delay = 1  # M√≠nimo delay con proxy
            self.empty_pages_threshold = 5  # M√°s p√°ginas para confirmar final
            self.batch_size = min(self.max_workers, 50)  # Lotes grandes
        else:
            # Modo sin proxy: conservador
            self.max_workers = max_workers or 3  # Muy limitado sin proxy
            self.retry_delay = 2  # Delay base m√°s alto
            self.empty_pages_threshold = 3
            self.batch_size = 2  # Lotes peque√±os
        
        # Headers espec√≠ficos para SkinOut (del BOT-vCSGO-Beta original)
        self.session.headers.update({
            'Accept': 'application/json',
            'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
            'Origin': 'https://skinout.gg',
            'Referer': 'https://skinout.gg/'
        })
        
        if self.logger:
            mode = "PROXY" if use_proxy else "SIN PROXY"
            proxy_info = f" ({proxy_count:,} proxies cargados)" if use_proxy and proxy_count > 0 else ""
            self.logger.info(f"üîß Cliente SkinOut inicializado en modo {mode}{proxy_info} - Workers: {self.max_workers}")
    
    def get_next_proxy(self):
        """
        Obtiene el siguiente proxy de la lista (rotaci√≥n circular del c√≥digo original)
        
        Returns:
            String del proxy en formato requerido por requests
        """
        if not self.proxy_list:
            return None
            
        proxy = self.proxy_list[self.current_proxy_index]
        self.current_proxy_index = (self.current_proxy_index + 1) % len(self.proxy_list)
        return proxy
    
    def log_proxy_usage(self, page, success=True, error_msg=None, current_proxy=None):
        """
        Log detallado del uso de proxies para monitoreo
        
        Args:
            page: N√∫mero de p√°gina
            success: Si la request fue exitosa
            error_msg: Mensaje de error si fall√≥
            current_proxy: Proxy actual usado
        """
        if not self.logger:
            return
            
        if self.use_proxy:
            # Mostrar proxy actual (ocultar credenciales si las hay)
            proxy_display = "N/A"
            if current_proxy:
                if '@' in current_proxy:
                    proxy_display = current_proxy.split('@')[-1]  # Solo IP:puerto
                else:
                    proxy_display = current_proxy
            
            status = "‚úÖ √âXITO" if success else "‚ùå ERROR"
            if success:
                self.logger.debug(f"üåê PROXY | P√°gina {page} | {status} | Proxy: {proxy_display} | √çndice: {self.current_proxy_index-1}/{len(self.proxy_list)}")
            else:
                self.logger.warning(f"üåê PROXY | P√°gina {page} | {status} | Proxy: {proxy_display} | Error: {error_msg}")
        else:
            status = "‚úÖ √âXITO" if success else "‚ùå ERROR"
            if success:
                self.logger.debug(f"üîó DIRECTO | P√°gina {page} | {status} | Sin proxy")
            else:
                self.logger.warning(f"üîó DIRECTO | P√°gina {page} | {status} | Error: {error_msg}")
    
    def obtener_datos_pagina(self, page):
        """
        Obtiene datos de una p√°gina espec√≠fica con reintentos indefinidos
        Integra el sistema de rotaci√≥n de proxies del c√≥digo original
        
        Args:
            page: N√∫mero de p√°gina a obtener
            
        Returns:
            Tupla (page, items) - nunca retorna None (reintentos indefinidos)
        """
        retry_count = 0
        base_delay = self.retry_delay
        
        while True:  # Reintentos indefinidos (como el c√≥digo original)
            current_proxy = None
            try:
                url = f"https://skinout.gg/api/market/items?page={page}"
                
                if self.logger and retry_count == 0:
                    self.logger.debug(f"üì° Fetching page {page} from SkinOut")
                
                # Configurar proxy si est√° habilitado (m√©todo del c√≥digo original)
                if self.use_proxy and self.proxy_list:
                    current_proxy = self.get_next_proxy()
                    proxies = {"http": current_proxy, "https": current_proxy}
                    
                    # Headers espec√≠ficos (del c√≥digo original)
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                        'Accept': 'application/json',
                        'Accept-Language': 'es-ES,es;q=0.9,en;q=0.8',
                        'Origin': 'https://skinout.gg',
                        'Referer': 'https://skinout.gg/'
                    }
                    
                    response = requests.get(
                        url,
                        proxies=proxies,
                        headers=headers,
                        timeout=15
                    )
                else:
                    # Sin proxy - usar la sesi√≥n normal
                    response = self.session.get(url, timeout=15)
                
                response.raise_for_status()
                json_data = response.json()
                def limpiar_texto(texto):
                    # Removemos los par√©ntesis y su contenido
                    texto = texto.replace("(", "").replace(")", "")

                    # Reemplazamos caracteres especiales y espacios con guiones
                    caracteres_especiales = ["|", " ", ".", ",", ";", ":", "!", "?", "'", '"', "‚Ñ¢"]
                    for caracter in caracteres_especiales:
                        texto = texto.replace(caracter, "-")

                    # Eliminamos guiones m√∫ltiples si se generaron
                    while "--" in texto:
                        texto = texto.replace("--", "-")

                    # Eliminamos guiones al inicio y final si existen
                    texto = texto.strip("-")

                    return texto
                if json_data.get('success') and 'items' in json_data:
                    items = json_data['items']
                    # Formatear items (del BOT-vCSGO-Beta original)
                    formatted_items = []
                    for item in items:
                        try:
                            formatted_items.append({
                                'Item': item['market_hash_name'],
                                'Price': float(item['price']),
                                'Platform': 'SkinOut',
                                'URL': "https://skinout.gg/en/market/" + limpiar_texto(item['market_hash_name'])
                            })
                        except Exception as e:
                            if self.logger:
                                self.logger.warning(f"Error procesando item en p√°gina {page}: {e}")
                            continue
                    
                    # Log √©xito con proxy
                    self.log_proxy_usage(page, success=True, current_proxy=current_proxy)
                    
                    # √âxito - resetear contador de reintentos si hab√≠a errores previos
                    if retry_count > 0 and self.logger:
                        mode_info = f"con {'proxy' if self.use_proxy else 'conexi√≥n directa'}"
                        self.logger.info(f"‚úÖ P√°gina {page} exitosa despu√©s de {retry_count} reintentos {mode_info}")
                    
                    return page, formatted_items
                
                # Si la respuesta es exitosa pero no hay items
                if json_data.get('success'):
                    self.log_proxy_usage(page, success=True, current_proxy=current_proxy)
                    return page, []
                
                # Si no hay success, intentar de nuevo
                raise Exception(f"Respuesta sin success en p√°gina {page}")
                
            except Exception as e:
                retry_count += 1
                
                # Log error con proxy
                self.log_proxy_usage(page, success=False, error_msg=str(e), current_proxy=current_proxy)
                
                # Calcular delay seg√∫n modo
                if self.use_proxy:
                    # Con proxy: delay fijo m√≠nimo, rotar proxy autom√°ticamente en cada error
                    delay = self.retry_delay
                    if "429" in str(e) or "too many" in str(e).lower():
                        if self.logger:
                            proxy_info = f"Rotando a proxy {self.current_proxy_index}/{len(self.proxy_list)}" if self.proxy_list else "sin proxies"
                            self.logger.warning(f"üö´ Error 429 en p√°gina {page} - {proxy_info}")
                        delay = 0.5  # Delay m√≠nimo para rotar proxy
                    
                    # La rotaci√≥n es autom√°tica en get_next_proxy() en el siguiente intento
                else:
                    # Sin proxy: backoff exponencial para rate limiting
                    if "429" in str(e) or "too many" in str(e).lower():
                        # Backoff exponencial m√°s agresivo para 429
                        delay = min(base_delay * (2 ** min(retry_count-1, 8)), 300)  # M√°ximo 5 minutos
                    else:
                        # Otros errores: backoff m√°s suave
                        delay = min(base_delay * (1.5 ** min(retry_count-1, 6)), 120)  # M√°ximo 2 minutos
                
                if self.logger:
                    mode_info = f"{'üåê PROXY' if self.use_proxy else 'üîó DIRECTO'}"
                    proxy_status = f" (proxy {self.current_proxy_index}/{len(self.proxy_list)})" if self.use_proxy and self.proxy_list else ""
                    self.logger.warning(f"{mode_info} | P√°gina {page} (intento {retry_count}){proxy_status}: {e}. Reintentando en {delay:.1f}s...")
                
                time.sleep(delay + random.uniform(0, 1))  # A√±adir jitter aleatorio
    
    def obtener_todas_las_paginas(self, start_page=1, max_pages=None):
        """
        Obtiene todas las p√°ginas usando paralelizaci√≥n optimizada seg√∫n modo
        
        Args:
            start_page: P√°gina inicial
            max_pages: M√°ximo n√∫mero de p√°ginas (None = sin l√≠mite)
            
        Returns:
            Lista completa de items de todas las p√°ginas
        """
        all_items = []
        current_page = start_page
        empty_pages_count = 0
        
        # L√≠mite autom√°tico seg√∫n modo
        if max_pages is None:
            max_pages = 10000 if self.use_proxy else 200  # M√°s p√°ginas con proxy
        
        mode_info = f"PROXY ({self.max_workers} workers)" if self.use_proxy else f"SIN PROXY ({self.max_workers} workers)"
        
        if self.logger:
            self.logger.info(f"Iniciando obtenci√≥n masiva - Modo: {mode_info} - P√°ginas: {start_page} a {max_pages}")
        
        while current_page <= max_pages and empty_pages_count < self.empty_pages_threshold:
            # Crear lote de p√°ginas a procesar
            pages_to_process = list(range(current_page, min(current_page + self.batch_size, max_pages + 1)))
            
            if self.logger:
                self.logger.info(f"Procesando lote: p√°ginas {pages_to_process[0]}-{pages_to_process[-1]} ({len(pages_to_process)} p√°ginas)")
            
            # Procesar lote con ThreadPoolExecutor
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                # Enviar todas las p√°ginas del lote
                future_to_page = {
                    executor.submit(self.obtener_datos_pagina, page): page 
                    for page in pages_to_process
                }
                
                # Recopilar resultados
                batch_items = 0
                for future in as_completed(future_to_page):
                    page = future_to_page[future]
                    try:
                        page_num, items = future.result()  # Nunca ser√° None ahora
                        if items:
                            all_items.extend(items)
                            batch_items += len(items)
                            empty_pages_count = 0  # Resetear contador
                        else:
                            empty_pages_count += 1
                            if self.logger:
                                self.logger.debug(f"P√°gina {page_num}: vac√≠a ({empty_pages_count}/{self.empty_pages_threshold})")
                    except Exception as e:
                        if self.logger:
                            self.logger.error(f"Error cr√≠tico procesando future de p√°gina {page}: {e}")
                        # No incrementar empty_pages_count por errores cr√≠ticos
            
            if self.logger and batch_items > 0:
                self.logger.info(f"Lote completado: +{batch_items} items (Total: {len(all_items)})")
            
            current_page += self.batch_size
            
            # Pausa entre lotes seg√∫n modo
            if current_page <= max_pages:
                pause = 0.5 if self.use_proxy else 3.0  # Pausa m√≠nima con proxy
                time.sleep(pause)
        
        if empty_pages_count >= self.empty_pages_threshold:
            if self.logger:
                self.logger.info(f"Scraping finalizado: {empty_pages_count} p√°ginas vac√≠as consecutivas detectadas")
        
        return all_items


class SkinoutScraper(BaseScraper):
    """
    Scraper para SkinOut.gg - Migrado desde BOT-vCSGO-Beta V2 Mejorado
    
    Mejoras V2.1:
    - Proxy por defecto (True) con soporte masivo para 230k proxies
    - Rate limiting inteligente sin proxy con backoff exponencial
    - Reintentos indefinidos con rotaci√≥n autom√°tica de proxy
    - Paralelizaci√≥n masiva (1000 workers) con proxy
    - Mode conservador sin proxy (3 workers)
    - Detecci√≥n inteligente de errores 429 y manejo espec√≠fico
    """
    
    def __init__(self, use_proxy: bool = True, proxy_list: Optional[List[str]] = None):
        """
        Inicializa el scraper de SkinOut
        
        Args:
            use_proxy: Si usar proxy (POR DEFECTO True)
            proxy_list: Lista de proxies a usar (se carga autom√°ticamente desde proxy.txt)
        """
        # Cargar proxies autom√°ticamente si no se proporcionan
        if proxy_list is None and use_proxy:
            print("üìÇ Cargando proxies desde BOT-VCSGO-BETA-V2/proxy.txt...")
            proxy_list = cargar_proxies_desde_archivo()
            
            if proxy_list:
                print(f"‚úÖ Proxies cargados exitosamente: {len(proxy_list):,} proxies disponibles")
            else:
                print("‚ö†Ô∏è No se pudieron cargar proxies. Continuando sin proxy...")
                use_proxy = False
        
        # Configuraci√≥n espec√≠fica seg√∫n modo proxy
        if use_proxy and proxy_list:
            config = {
                'timeout': 15,
                'max_retries': 999,  # Reintentos indefinidos
                'retry_delay': 1,    # M√≠nimo con proxy
                'interval': 60,      # Menor intervalo con proxy
                'headers': {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept-Encoding': 'gzip, deflate, br'
                }
            }
        else:
            # Sin proxy o sin lista de proxies
            use_proxy = False
            proxy_list = None
            config = {
                'timeout': 15,
                'max_retries': 999,  # Reintentos indefinidos tambi√©n
                'retry_delay': 2,    # Mayor delay base sin proxy
                'interval': 300,     # Intervalo m√°s largo sin proxy
                'headers': {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                    'Accept-Encoding': 'gzip, deflate, br'
                }
            }
        
        super().__init__('skinout', use_proxy, proxy_list, config)
        
        # Contar proxies para logs
        proxy_count = len(proxy_list) if proxy_list else 0
        
        # Inicializar cliente SkinOut usando la sesi√≥n del BaseScraper
        self.client = SkinoutClient(
            self.session, 
            logger=self.logger, 
            use_proxy=use_proxy,
            proxy_count=proxy_count,
            proxy_list=proxy_list  # Pasar la lista para rotaci√≥n manual
        )
        
        mode_info = "PROXY" if use_proxy else "SIN PROXY"
        self.logger.info(f"üöÄ SkinOut scraper V2.1 inicializado en modo {mode_info}")
        
        if use_proxy and proxy_list:
            self.logger.info(f"üåê Sistema de proxies configurado: {len(proxy_list):,} proxies cargados desde proxy.txt")
            # Log de muestra de proxies (primeros 3, sin mostrar credenciales completas)
            sample_proxies = []
            for proxy in proxy_list[:3]:
                # Ocultar credenciales si las hay
                if '@' in proxy:
                    proxy_clean = proxy.split('@')[-1]  # Solo IP:puerto
                else:
                    proxy_clean = proxy
                sample_proxies.append(proxy_clean)
            self.logger.info(f"üîç Muestra de proxies: {', '.join(sample_proxies)}...")
        else:
            self.logger.info("üîó Modo conexi√≥n directa - Sin proxies")
    
    def parse_response(self, response) -> List[Dict]:
        """
        M√©todo requerido por BaseScraper - no usado directamente en SkinOut
        """
        return []
    
    def fetch_data(self) -> List[Dict]:
        """
        Obtiene datos de SkinOut usando paginaci√≥n paralela optimizada
        
        Mejoras V2.1:
        - Paralelizaci√≥n masiva con proxy (1000 workers)
        - Rate limiting inteligente sin proxy (3 workers)
        - Reintentos indefinidos con backoff exponencial
        - Rotaci√≥n autom√°tica de proxies en errores 429
        - NUEVO: Logs detallados del uso de proxies
        
        Returns:
            Lista completa de items del marketplace
        """
        proxy_count = len(self.proxy_list) if self.proxy_list else 0
        mode_info = f"PROXY ({proxy_count:,} proxies)" if self.client.use_proxy else "SIN PROXY (rate limited)"
        self.logger.info(f"üéØ Iniciando scraping masivo de SkinOut - Modo: {mode_info}")
        
        # Log sobre API key
        if self.api_key:
            self.logger.info("üîë Usando API key de SkinOut para autenticaci√≥n")
        
        # Log inicial de configuraci√≥n de proxy
        if self.client.use_proxy and proxy_count > 0:
            self.logger.info(f"üåê Configuraci√≥n de proxy activa:")
            self.logger.info(f"   üìä Total proxies: {proxy_count:,}")
            self.logger.info(f"   üöÄ Workers: {self.client.max_workers}")
            self.logger.info(f"   üì¶ Tama√±o de lote: {self.client.batch_size}")
            self.logger.info(f"   ‚è±Ô∏è Delay entre reintentos: {self.client.retry_delay}s")
        else:
            self.logger.info(f"üîó Configuraci√≥n sin proxy:")
            self.logger.info(f"   üêå Workers limitados: {self.client.max_workers}")
            self.logger.info(f"   üì¶ Tama√±o de lote: {self.client.batch_size}")
            self.logger.info(f"   ‚è±Ô∏è Delay base: {self.client.retry_delay}s (con backoff exponencial)")
        
        try:
            # Obtener todas las p√°ginas usando el cliente optimizado
            all_items = self.client.obtener_todas_las_paginas(
                start_page=1,
                max_pages=None  # Sin l√≠mite, se detiene por p√°ginas vac√≠as
            )
            
            if all_items:
                self.logger.info(f"‚úÖ Scraping completado exitosamente. Total items: {len(all_items):,}")
                
                # Log final de estad√≠sticas de proxy si aplica
                if self.client.use_proxy:
                    self.logger.info(f"üåê Estad√≠sticas finales de proxy:")
                    self.logger.info(f"   üì° Requests completadas usando {proxy_count:,} proxies disponibles")
                    self.logger.info(f"   ‚ö° Modo paralelo masivo con {self.client.max_workers} workers")
            else:
                self.logger.warning("‚ö†Ô∏è No se obtuvieron items en esta ejecuci√≥n")
            
            return all_items
            
        except Exception as e:
            self.logger.error(f"‚ùå Error cr√≠tico en fetch_data de SkinOut: {e}")
            return []


def main():
    """
    Funci√≥n principal para ejecutar el scraper
    NUEVO: Proxy por defecto con opci√≥n de desactivar
    """
    print("=== SkinOut Scraper V2.1 - Modo Proxy por Defecto ===")
    print("Cambios principales:")
    print("- ‚úÖ Proxy activado por defecto (230k proxies disponibles)")
    print("- ‚úÖ Paralelizaci√≥n masiva: 1000 workers con proxy / 3 sin proxy")
    print("- ‚úÖ Reintentos indefinidos con backoff inteligente")
    print("- ‚úÖ Rotaci√≥n autom√°tica de proxies en errores 429")
    print("- ‚úÖ Rate limiting inteligente sin proxy")
    
    # Configuraci√≥n de proxy
    use_proxy_input = input("\n¬øUsar proxy? (Y/n): ").lower()
    use_proxy = use_proxy_input != 'n'  # Por defecto True
    
    # Crear scraper
    scraper = SkinoutScraper(use_proxy=use_proxy)
    
    try:
        print(f"\nüöÄ Ejecutando SkinOut Scraper V2.1...")
        mode = "PROXY (masivo)" if use_proxy else "SIN PROXY (limitado)"
        print(f"üì° Modo: {mode}")
        
        # Ejecutar una vez para prueba
        start_time = time.time()
        data = scraper.run_once()
        end_time = time.time()
        
        print(f"\n‚úÖ Scraper completado en {end_time - start_time:.1f} segundos:")
        print(f"   üìä Items obtenidos: {len(data):,}")
        print(f"   üìà Estad√≠sticas: {scraper.get_stats()}")
        
        if data:
            print(f"\nüìã Muestra de items (primeros 5):")
            for i, item in enumerate(data[:5], 1):
                print(f"   {i}. {item['Item']}: ${item['Price']}")
                
            # Mostrar rentabilidad si est√° disponible
            profitable_items = [item for item in data if item.get('is_profitable', False)]
            if profitable_items:
                print(f"\nüí∞ Items rentables encontrados: {len(profitable_items)}")
                for i, item in enumerate(profitable_items[:3], 1):
                    profit_info = f"Ganancia: {item.get('profit_margin', 0):.1f}%" if item.get('profit_margin') else "Ganancia: calculando..."
                    print(f"   {i}. {item['Item']}: ${item['Price']} ‚Üí ${item.get('sell_price', 'N/A')} ({profit_info})")
        
        # Opci√≥n para ejecutar en bucle
        run_forever = input("\nüîÑ ¬øEjecutar en bucle continuo? (y/N): ").lower() == 'y'
        if run_forever:
            print("üîÅ Iniciando bucle infinito... (Ctrl+C para detener)")
            interval = 60 if use_proxy else 300  # Intervalo seg√∫n modo
            print(f"‚è±Ô∏è Intervalo entre ejecuciones: {interval} segundos")
            scraper.run_forever()
            
    except KeyboardInterrupt:
        print("\nüõë Detenido por el usuario")
    except Exception as e:
        print(f"\n‚ùå Error cr√≠tico: {e}")
        import traceback
        traceback.print_exc()
    finally:
        # Limpiar recursos
        print("\nüßπ Cerrando conexiones...")
        scraper.session.close()
        print("‚úÖ Limpieza completada")


if __name__ == "__main__":
    main()