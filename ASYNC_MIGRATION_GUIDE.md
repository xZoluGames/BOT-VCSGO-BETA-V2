# üöÄ Gu√≠a de Migraci√≥n a Async - BOT-VCSGO-BETA-V2

## üìã Resumen de Optimizaciones de Rendimiento

### ‚úÖ Componentes Implementados

1. **Base Scraper As√≠ncrono** (`core/async_base_scraper.py`)
   - Soporte async/await nativo
   - Connection pooling autom√°tico
   - Rate limiting inteligente
   - M√©tricas de rendimiento

2. **Cache Service As√≠ncrono** (`core/async_cache_service.py`)
   - TTL adaptativo basado en frecuencia
   - Compresi√≥n autom√°tica
   - Estad√≠sticas de uso
   - Cache en memoria y disco

3. **Ejemplo de Scraper As√≠ncrono** (`scrapers/async_waxpeer_scraper.py`)
   - Procesamiento paralelo de p√°ginas
   - Validaci√≥n as√≠ncrona

4. **Runner As√≠ncrono** (`run_async.py`)
   - Ejecuci√≥n paralela de m√∫ltiples scrapers
   - M√©tricas y comparaci√≥n de rendimiento

## üìä Beneficios Esperados

- **‚ö° 60-80% m√°s r√°pido** en operaciones de red
- **üîÑ Concurrencia real** - m√∫ltiples requests simult√°neos
- **üíæ Menor uso de memoria** con connection pooling
- **üìà Mayor throughput** - m√°s items por segundo
- **üõ°Ô∏è M√°s resiliente** - mejor manejo de errores

## üîß Gu√≠a de Migraci√≥n Paso a Paso

### Paso 1: Instalar Dependencias As√≠ncronas

```bash
pip install aiohttp aiofiles asyncio orjson
```

### Paso 2: Estructura de un Scraper As√≠ncrono

Aqu√≠ est√° la plantilla para convertir cualquier scraper:

```python
"""
Async [Platform] Scraper - BOT-VCSGO-BETA-V2
"""

import asyncio
from typing import List, Dict, Any, Optional
from core.async_base_scraper import AsyncBaseScraper

class Async[Platform]Scraper(AsyncBaseScraper):
    def __init__(self, use_proxy: Optional[bool] = None):
        # Configuraci√≥n espec√≠fica de la plataforma
        custom_config = {
            'rate_limit': 60,      # Ajustar seg√∫n API
            'burst_size': 10,      
            'cache_ttl': 300,      
            'timeout_seconds': 30,
            'max_retries': 3
        }
        
        super().__init__(
            platform_name='[platform]',
            use_proxy=use_proxy,
            custom_config=custom_config
        )
        
        # URLs y configuraci√≥n espec√≠fica
        self.api_url = "https://api.[platform].com/v1/items"
    
    async def scrape(self) -> List[Dict[str, Any]]:
        """Implementaci√≥n del scraping"""
        # Tu l√≥gica de scraping aqu√≠
        items = await self._fetch_items()
        return await self._process_items(items)
    
    async def _fetch_items(self) -> List[Dict[str, Any]]:
        """Obtiene items de la API"""
        # Usar self.fetch_json() en lugar de requests.get()
        data = await self.fetch_json(self.api_url)
        return data.get('items', [])
    
    async def _process_items(self, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Procesa items en paralelo"""
        tasks = [self._process_single_item(item) for item in items]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filtrar errores
        return [r for r in results if not isinstance(r, Exception)]
```

### Paso 3: Conversi√≥n de C√≥digo S√≠ncrono a As√≠ncrono

#### üîÑ Requests ‚Üí Aiohttp

**Antes (S√≠ncrono):**
```python
import requests

response = requests.get(url, headers=headers)
data = response.json()
```

**Despu√©s (As√≠ncrono):**
```python
# Usando el m√©todo helper del base scraper
data = await self.fetch_json(url, headers=headers)

# O manualmente:
response = await self.fetch(url, headers=headers)
data = await response.json()
```

#### üîÑ Procesamiento Secuencial ‚Üí Paralelo

**Antes (S√≠ncrono):**
```python
processed_items = []
for item in items:
    processed = self.process_item(item)
    if processed:
        processed_items.append(processed)
```

**Despu√©s (As√≠ncrono):**
```python
# Crear tareas para procesamiento paralelo
tasks = [self.process_item(item) for item in items]
results = await asyncio.gather(*tasks, return_exceptions=True)

# Filtrar resultados v√°lidos
processed_items = [r for r in results if r and not isinstance(r, Exception)]
```

#### üîÑ File I/O ‚Üí Async File I/O

**Antes (S√≠ncrono):**
```python
with open('data.json', 'w') as f:
    json.dump(data, f)
```

**Despu√©s (As√≠ncrono):**
```python
import aiofiles
import orjson

async with aiofiles.open('data.json', 'wb') as f:
    await f.write(orjson.dumps(data))
```

#### üîÑ Sleep ‚Üí Async Sleep

**Antes (S√≠ncrono):**
```python
time.sleep(1)
```

**Despu√©s (As√≠ncrono):**
```python
await asyncio.sleep(1)
```

### Paso 4: Manejo de Rate Limiting

El base scraper as√≠ncrono incluye rate limiting autom√°tico:

```python
# En tu __init__:
custom_config = {
    'rate_limit': 120,  # Requests por minuto
    'burst_size': 20,   # Requests m√°ximos en burst
}

# El rate limiter se encarga autom√°ticamente
# No necesitas implementar delays manuales
```

### Paso 5: Cache Inteligente

El cache as√≠ncrono se usa autom√°ticamente:

```python
# Cache autom√°tico con fetch_json
data = await self.fetch_json(url)  # Primer request: API
data = await self.fetch_json(url)  # Segundo request: Cache

# Control manual del cache
await self.cache.set('key', value, ttl=600)
value = await self.cache.get('key')
```

### Paso 6: Validaci√≥n As√≠ncrona

```python
async def validate_item(self, item: Dict[str, Any]) -> bool:
    # Validaci√≥n base
    if not await super().validate_item(item):
        return False
    
    # Validaciones espec√≠ficas de la plataforma
    # Puedes hacer validaciones as√≠ncronas aqu√≠
    if need_external_validation:
        is_valid = await self._check_external_api(item)
        return is_valid
    
    return True
```

## üìù Ejemplo Completo: Migraci√≥n de Empire Scraper

### Version Original (S√≠ncrona)
```python
class EmpireScraper(BaseScraper):
    def scrape_market(self):
        response = requests.get(self.api_url, headers=self.headers)
        items = response.json()['items']
        
        processed = []
        for item in items:
            if self.validate_item(item):
                processed.append(self.process_item(item))
        
        return processed
```

### Version Migrada (As√≠ncrona)
```python
class AsyncEmpireScraper(AsyncBaseScraper):
    async def scrape(self) -> List[Dict[str, Any]]:
        # Fetch con cache autom√°tico
        data = await self.fetch_json(self.api_url)
        items = data.get('items', [])
        
        # Procesamiento paralelo
        tasks = []
        for item in items:
            task = asyncio.create_task(self._process_single_item(item))
            tasks.append(task)
        
        # Ejecutar todas las tareas en paralelo
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Filtrar resultados v√°lidos
        return [r for r in results if r and not isinstance(r, Exception)]
    
    async def _process_single_item(self, item: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        if not await self.validate_item(item):
            return None
        
        return {
            'name': item['name'],
            'price': item['price'] / 100,  # Centavos a d√≥lares
            'quantity': item['quantity'],
            # ... m√°s campos
        }
```

## üöÄ Ejecutar Scrapers As√≠ncronos

### Individual
```bash
# Ejecutar un scraper espec√≠fico
python -m scrapers.async_waxpeer_scraper

# Con comparaci√≥n de rendimiento
python scrapers/async_waxpeer_scraper.py
```

### M√∫ltiples Scrapers
```bash
# Ejecutar todos los scrapers as√≠ncronos
python run_async.py

# Ejecutar scrapers espec√≠ficos
python run_async.py waxpeer empire skinport

# Con configuraci√≥n avanzada
python run_async.py --concurrent 10 --use-proxy

# Comparaci√≥n sync vs async
python run_async.py --compare

# Test de estr√©s
python run_async.py --stress-test --stress-iterations 10
```

## üìä M√©tricas de Rendimiento

El sistema incluye m√©tricas autom√°ticas:

```python
# Acceder a m√©tricas despu√©s del scraping
metrics = scraper.metrics.to_dict()
print(f"Requests exitosos: {metrics['requests_successful']}")
print(f"Tiempo promedio: {metrics['avg_response_time']}")
print(f"Cache hit rate: {metrics['cache_hit_rate']}")
```

## ‚ö° Tips de Optimizaci√≥n

### 1. **Ajustar Concurrencia**
```python
# En el runner
runner = AsyncScraperRunner(max_concurrent=10)  # M√°s scrapers paralelos

# En el scraper individual
self.connector = aiohttp.TCPConnector(
    limit=100,           # Total de conexiones
    limit_per_host=30    # Conexiones por host
)
```

### 2. **Optimizar Batch Processing**
```python
# Procesar en lotes para APIs con l√≠mites
async def _fetch_all_pages(self):
    batch_size = 5
    for i in range(0, total_pages, batch_size):
        batch = [self._fetch_page(j) for j in range(i, min(i+batch_size, total_pages))]
        results = await asyncio.gather(*batch)
        # Procesar resultados
        await asyncio.sleep(0.5)  # Pausa entre lotes
```

### 3. **Cache Estrat√©gico**
```python
# TTL adaptativo basado en frecuencia de cambio
if frequently_changing_data:
    ttl = 60  # 1 minuto
else:
    ttl = 3600  # 1 hora
```

### 4. **Manejo de Errores Robusto**
```python
async def fetch_with_retry(self, url: str, max_retries: int = 3):
    for attempt in range(max_retries):
        try:
            return await self.fetch_json(url)
        except Exception as e:
            if attempt == max_retries - 1:
                raise
            await asyncio.sleep(2 ** attempt)  # Backoff exponencial
```

## üéØ Checklist de Migraci√≥n

- [ ] Instalar dependencias as√≠ncronas
- [ ] Crear clase que herede de `AsyncBaseScraper`
- [ ] Convertir m√©todos a `async def`
- [ ] Reemplazar `requests` con `aiohttp`
- [ ] Cambiar procesamiento secuencial a paralelo
- [ ] Actualizar file I/O a versi√≥n as√≠ncrona
- [ ] Ajustar rate limiting y configuraci√≥n
- [ ] Probar y comparar rendimiento
- [ ] Actualizar documentaci√≥n

## üîç Troubleshooting

### Error: "RuntimeError: This event loop is already running"
**Soluci√≥n**: Usa `asyncio.create_task()` en lugar de `asyncio.run()` dentro de funciones async.

### Error: "Session is closed"
**Soluci√≥n**: Aseg√∫rate de usar el context manager (`async with`) o llamar `setup()` y `cleanup()`.

### Rendimiento no mejora
**Verificar**:
- ¬øEst√°s procesando en paralelo?
- ¬øEl rate limiting es muy restrictivo?
- ¬øHay bloqueos s√≠ncronos en el c√≥digo?

## üìà Resultados Esperados

Con la migraci√≥n a async, deber√≠as ver:

- **Waxpeer**: ~2-3x m√°s r√°pido
- **Empire**: ~2-4x m√°s r√°pido
- **Multiple scrapers**: ~5-10x m√°s r√°pido (ejecuci√≥n paralela)

El mayor beneficio se ve cuando:
- Ejecutas m√∫ltiples scrapers simult√°neamente
- La API permite muchos requests paralelos
- Hay mucha latencia de red

¬°Tu bot ahora es significativamente m√°s r√°pido y eficiente! üöÄ